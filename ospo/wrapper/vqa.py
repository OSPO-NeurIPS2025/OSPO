import os
import torch
from pytorch_lightning import LightningModule

import pyrootutils
pyrootutils.setup_root(__file__, indicator=".project-root", pythonpath=True, cwd=True)
from ospo.utils.common import save_json_ddp
from ospo.utils.processor import get_sft_format
from ospo.prompt.template_vqa import get_vqa_prompt


class JanusProVQAWrapper(LightningModule):
    def __init__(self, config, model, tokenizer, processor, mode='question'):
        super().__init__()
        self.config=config
        self.model=model
        self.tokenizer=tokenizer
        self.processor=processor
        if mode not in ['question', 'answer']:
            raise ValueError("Mode must be either 'question' or 'answer'.")
        self.mode = mode
        self.output_list = []
        self.global_question = "This image is generated by a prompt: {}. Does this image accurately represent the prompt?"


    def on_test_epoch_start(self):
        self.model.eval() 
    

    @torch.inference_mode()
    def test_step(self, batch, batch_idx):
        if self.mode == 'question':
            sft_format_list = []
            for sample in batch:
                system_prompt, conversation = get_vqa_prompt(sample['category'], sample['prompt'])
                sft_format = get_sft_format(self.processor, system_prompt, conversation)
                sft_format_list.append(sft_format)

            input_embeds, attention_mask = self.get_input_embeds(sft_format_list)
            outputs = self.generate(input_embeds, attention_mask)

            # decode and save the output
            self.decode(batch, outputs)


    @torch.inference_mode()
    def generate(self, input_embeds, attention_mask):
        generation_config = self.config.generation_config
        outputs = self.model.language_model.generate(inputs_embeds=input_embeds,
            attention_mask=attention_mask,
            pad_token_id=self.tokenizer.eos_token_id,
            bos_token_id=self.tokenizer.bos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            use_cache=True,
            **generation_config
            )

        return outputs
    

    def decode(self, batch, outputs):
        for sample, output in zip(batch, outputs):
            answer = self.tokenizer.decode(output.cpu().tolist(), skip_special_tokens=True)
            answer = answer.split("Questions: ")[-1]
            questions = [t.strip() + '?' for t in answer.split('?') if t.strip().rstrip('.')]

            # add global question
            questions.append(self.global_question.format(sample['prompt'].rstrip('.')))
            sample['question'] = questions
            self.output_list.append(sample)


    def get_input_embeds(self, sft_format_list):
        # for padding
        input_ids_list = []
        seq_length_list = []
        
        for prompt in sft_format_list:      
            input_ids = self.tokenizer.encode(prompt)
            input_ids = torch.LongTensor(input_ids)
            input_ids_list.append(input_ids)
            seq_length_list.append(len(input_ids))
            
        max_len = max(seq_length_list)
        batch_size = len(sft_format_list)

        tokens = torch.zeros((batch_size, max_len), dtype=torch.int).to(self.device)
        attention_mask = torch.ones((batch_size, max_len), dtype=torch.long).to(self.device)
        for i, (length, input_ids) in enumerate(zip(seq_length_list, input_ids_list)):
            pad_len = max_len - length
            tokens[i, pad_len:] = input_ids
            tokens[i, :pad_len] = self.processor.pad_id
            attention_mask[i, :pad_len] = 0        
        input_embeds = self.model.language_model.get_input_embeddings()(tokens)  
        
        return input_embeds, attention_mask


    def on_test_epoch_end(self):
        save_json_ddp(
            save_root=self.config.save_path,
            save_name='vqa_prompt' if self.mode == 'question' else 'vqa_answer', # TODO
            world_size=self.trainer.world_size,
            save_file=self.output_list,
            rank=self.trainer.global_rank,
        )
        print("Saved VQA done.")

