import os
import numpy as np
from glob import glob
from PIL import Image
import torch
from pytorch_lightning import LightningModule

import pyrootutils
pyrootutils.setup_root(__file__, indicator=".project-root", pythonpath=True, cwd=True)
from ospo.utils.common import save_json_ddp
from ospo.utils.processor import get_sft_format
from ospo.templates import get_vqa_prompt


class JanusProQuestionGenWrapper(LightningModule):
    def __init__(self, config, model, tokenizer, processor):
        super().__init__()
        self.config=config
        self.model=model
        self.tokenizer=tokenizer
        self.processor=processor

        self.output_list = []
        self.global_question = "This image is generated by a prompt: {}. Does this image accurately represent the prompt?"


    def on_test_epoch_start(self):
        self.model.eval() 
    

    @torch.inference_mode()
    def test_step(self, batch, batch_idx):
        sft_format_list = []
        for sample in batch:
            system_prompt, conversation = get_vqa_prompt(sample['category'], sample['prompt'])
            sft_format = get_sft_format(self.processor, system_prompt, conversation)
            sft_format_list.append(sft_format)

        input_embeds, attention_mask = self.get_input_embeds(sft_format_list)
        outputs = self.generate(input_embeds, attention_mask)

        # decode and save the output
        self.decode(batch, outputs)


    @torch.inference_mode()
    def generate(self, input_embeds, attention_mask):
        generation_config = self.config.generation_config
        outputs = self.model.language_model.generate(inputs_embeds=input_embeds,
            attention_mask=attention_mask,
            pad_token_id=self.tokenizer.eos_token_id,
            bos_token_id=self.tokenizer.bos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            use_cache=True,
            **generation_config
            )

        return outputs
    

    def decode(self, batch, outputs):
        for sample, output in zip(batch, outputs):
            answer = self.tokenizer.decode(output.cpu().tolist(), skip_special_tokens=True)
            answer = answer.split("Questions: ")[-1]
            questions = [t.strip() + '?' for t in answer.split('?') if t.strip().rstrip('.')]

            # add global question
            questions.append(self.global_question.format(sample['prompt'].rstrip('.')))
            sample['question'] = questions
            self.output_list.append(sample)


    def get_input_embeds(self, sft_formats):
        # for padding
        input_ids_list = []
        seq_length_list = []
        
        for prompt in sft_formats:      
            input_ids = self.tokenizer.encode(prompt)
            input_ids = torch.LongTensor(input_ids)
            input_ids_list.append(input_ids)
            seq_length_list.append(len(input_ids))
            
        max_len = max(seq_length_list)
        batch_size = len(sft_formats)

        tokens = torch.zeros((batch_size, max_len), dtype=torch.int).to(self.device)
        attention_mask = torch.ones((batch_size, max_len), dtype=torch.long).to(self.device)
        for i, (length, input_ids) in enumerate(zip(seq_length_list, input_ids_list)):
            pad_len = max_len - length
            tokens[i, pad_len:] = input_ids
            tokens[i, :pad_len] = self.processor.pad_id
            attention_mask[i, :pad_len] = 0        
        input_embeds = self.model.language_model.get_input_embeddings()(tokens)  
        
        return input_embeds, attention_mask


    def on_test_epoch_end(self):
        save_json_ddp(
            save_root=self.config.save_path,
            save_name='vqa_prompt',
            world_size=self.trainer.world_size,
            save_file=self.output_list,
            rank=self.trainer.global_rank,
        )
        print("Saved VQA question done.")




class JanusProScoreWrapper(LightningModule):
    def __init__(self, config, model, tokenizer, processor):
        super().__init__()
        self.config=config
        self.model=model
        self.tokenizer=tokenizer
        self.processor=processor

        self.output_list = []
        self.yes_ids = [self.tokenizer("yes", add_special_tokens=False).input_ids[-1],
                        self.tokenizer("Yes", add_special_tokens=False).input_ids[-1]]
        self.no_ids  = [self.tokenizer("no", add_special_tokens=False).input_ids[-1],
                        self.tokenizer("No", add_special_tokens=False).input_ids[-1]]


    def on_test_epoch_start(self):
        self.model.eval() 


    @torch.inference_mode()
    def test_step(self, batch, batch_idx):
        questions_batched = [sample['question'] for sample in batch]  
        base_paths_batched, negative_paths_batched = [], []

        for sample in batch:
            base_paths = sorted(glob(os.path.join(self.config.image_path, 'base', sample['category'], sample['item_id'], '*.png')))
            negative_paths = sorted(glob(os.path.join(self.config.image_path, 'negative', sample['category'], sample['item_id'], '*.png')))
            base_paths_batched.append(base_paths)
            negative_paths_batched.append(negative_paths)

        base_img_metadata_batched = self.get_score_single(base_paths_batched, questions_batched)
        negative_img_metadata_batched = self.get_score_single(negative_paths_batched, questions_batched)

        # select pair by (preference strength) score
        self.select_pair(batch, base_img_metadata_batched, negative_img_metadata_batched)


    def build_conversation(self, img, questions):
        convs = []
        for q in questions:
            convs.append([
                {"role": "<|User|>",
                "content": f"<image_placeholder>\n{q} Please answer 'yes' or 'no' without explanation.",
                "images": [img]},
                {"role": "<|Assistant|>", "content": ""}
            ])
        return convs, [[img]] * len(questions)


    def get_score_single(self, img_paths_batched, questions_batched):
        img_metadata_batched = []

        for sample_idx, img_paths in enumerate(img_paths_batched):
            sample_metadata = {}

            for img_idx, img_path in enumerate(img_paths):
                with Image.open(img_path) as img:
                    convs, imgs = self.build_conversation(img, questions_batched[sample_idx])
                    logits = self.forward_single(convs, imgs)  # shape: [num_questions, seq_len, vocab]
                    probs = torch.softmax(logits[:, -1, :], dim=-1)

                    score_sum = 0
                    answer_metadata = []
                    q_count = len(questions_batched[sample_idx])

                    for q_idx in range(q_count):
                        p_yes = max(probs[q_idx, y].item() for y in self.yes_ids)
                        p_no = max(probs[q_idx, n].item() for n in self.no_ids)

                        answer_metadata.append({
                            'p_yes': float(p_yes),
                            'p_no': float(p_no),
                            'answer': 'yes' if p_yes > p_no else ('no' if p_no > p_yes else 'tie')
                        })

                        if q_idx == q_count - 1:
                            global_score = p_yes - p_no
                        else:
                            score_sum += (p_yes - p_no)

                    local_score = score_sum / (q_count - 1)
                    prefix = 'base' if 'base' in img_path else 'negative'

                    sample_metadata[f'{prefix}_{img_idx}'] = {
                        'path': img_path,
                        'local_score': float(local_score),
                        'global_score': float(global_score),
                        'answer_metadata': answer_metadata
                    }

            img_metadata_batched.append(sample_metadata)

        return img_metadata_batched

    
    def forward_single(self, convs, imgs):
        prepare_list = []
        for conv, img in zip(convs, imgs):
            prepare = self.processor.process_one(
                conversations=conv,
                images=img,
                force_batchify=True
            )
            prepare_list.append(prepare)

        with torch.no_grad():
            batch_inputs = self.processor.batchify(prepare_list).to(self.device)
            inputs_embeds = self.model.prepare_inputs_embeds(**batch_inputs)
            outputs = self.model.language_model(
                inputs_embeds=inputs_embeds,
                attention_mask=batch_inputs.attention_mask
            )

        return outputs.logits


    def compute_preference_strength(self, base_img_dict, negative_img_dict):
        # For each sample, we assume that length of base and negative have same length.
        bases = [(i, base_img_dict.get(f'base_{i}')) for i in range(3) if base_img_dict.get(f'base_{i}') is not None]
        negatives = [(i, negative_img_dict.get(f'negative_{i}')) for i in range(3) if negative_img_dict.get(f'negative_{i}') is not None]
        if len(bases) == 0 or len(negatives) == 0:
            return None

        pairs = []
        for idx in range(3):  
            base = base_img_dict.get(f'base_{idx}')
            neg = negative_img_dict.get(f'negative_{idx}')

            if base is not None and neg is not None:
                local_gap = base['local_score'] - neg['local_score']
                global_gap = base['global_score'] - neg['global_score']

                # filter
                if local_gap >= 0 and global_gap >= 0:
                    pairs.append({
                        'pair_idx': idx,
                        'local_gap': local_gap,
                        'global_gap': global_gap
                    })
        if not pairs: 
            return None

        # For normalization,
        max_local_gap = max(abs(p['local_gap']) for p in pairs)
        max_global_gap = max(abs(p['global_gap']) for p in pairs)

        best_score = -np.inf
        best_pair = None

        for pair in pairs:
            norm_local = abs(pair['local_gap']) / (max_local_gap + 1e-8)
            norm_global = abs(pair['global_gap']) / (max_global_gap + 1e-8)
            preference_strength = norm_local / (norm_global + 1e-8)  # Avoid division by zero

            if preference_strength > best_score:
                best_score = preference_strength
                best_pair = pair
        if best_pair is None:
            return None
        
        chosen = base_img_dict[f'base_{best_pair["pair_idx"]}']['path']
        rejected = negative_img_dict[f'negative_{best_pair["pair_idx"]}']['path']
        score_metadata = {
            "local_gap": best_pair["local_gap"],
            "global_gap": best_pair["global_gap"],
            "preference_strength": best_score,
        }

        return (chosen, rejected, score_metadata)
    

    def select_pair(self, batch, base_metadata_batched, negative_metadata_batched):
        for sample_idx, (base_dict, negative_dict) in enumerate(zip(base_metadata_batched, negative_metadata_batched)):
            # Compute preference strength
            result = self.compute_preference_strength(base_dict, negative_dict)
            if result is None:
                continue
            else:
                chosen, rejected, score_metadata = result
            
            # Prepare training dataset
            output = {
                "item_id": batch[sample_idx]['item_id'],
                "category": batch[sample_idx]['category'],
                "sub_category": batch[sample_idx]['sub_category'],
                "question": batch[sample_idx]['question'],
                "prompt": batch[sample_idx]['prompt'],
                "chosen": chosen,
                "rejected": rejected,
                "metadata": { 
                    "score_metadata": score_metadata,
                    "base_meatadata": base_dict,
                    "negative_metadata": negative_dict
                }
            }
            self.output_list.append(output)


    def on_test_epoch_end(self):
        save_json_ddp(
            save_root=self.config.save_path,
            save_name='train',
            world_size=self.trainer.world_size,
            save_file=self.output_list,
            rank=self.trainer.global_rank,
        )
        print("Saved Train dataset done.")